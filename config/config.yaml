#LogisticRegressionModel:
#  evaluate:
#    flag: True
#    evaluator_config:
#      explainability_algorithm: permutation
#      metric_prefix: evaluation_
#  fine_tune:
#    flag: True
#    strategy: BayesSearch
#    param_grid:
#      penalty: Categorical( [ 'l1', 'l2' ] )
#      C: Real( 1e-04, 1e+04 )
#      solver: Categorical( [ 'saga', 'liblinear' ] )
#      tol: Real( 1e-04, 1e-01 )

RandomForestModel:
  evaluate:
    flag: True
    evaluator_config:
      explainability_algorithm: permutation
      metric_prefix: evaluation_
  fine_tune:
    flag: True
    strategy: RandomSearch
    param_grid:
      n_estimators: list(range(10, 100, 10))
      max_features: list(range(6, 31, 5))
      criterion: list(['gini', 'entropy'])
      max_depth: list([8, 16, 32, 64])

LightGBMModel:
  evaluate:
    flag: True
    evaluator_config:
      explainability_algorithm: permutation
      metric_prefix: evaluation_
  fine_tune:
    flag: True
    strategy: RandomSearch
    param_grid:
      learning_rate: [ 0.1, 0.01 ]
      n_estimators: [ 50, 100, 150 ]
      max_depth: [ 4, 6 ]
      colsample_bytree: [ 0.7, 0.8, 0.9 ]
      subsample: [ 0.7, 0.8, 0.9 ]
      min_child_samples: [ 1, 5, 10 ]
  creation:
    boosting_type: gbdt
    objective: binary
    num_iterations: 2000
    learning_rate: 0.01
    metric: auc

NeuralNetworkModel:
 creation:
   layers_units: [ 8 , 16 , 8 ]
   activation: relu
   dropout: 0.3
 compilation:
   optimizer: adam
   loss: binary_crossentropy
   metrics: [ accuracy ]
 fit:
   batch_size: 64
   epochs: 10
   use_multiprocessing: True
 fine_tune:
   flag: False
