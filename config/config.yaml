LogisticRegressionModel:
  evaluate:
    flag: True
    evaluator_config:
      explainability_algorithm: permutation
      metric_prefix: evaluation_
  fine_tune:
    flag: True
    strategy: RandomSearch
    param_grid:
      penalty: list( [ 'l1', 'l2' ] )
      C: np.linspace(1e-04, 1e+04, 100)
      solver: list( [ 'saga', 'liblinear' ] )
      tol: np.linspace(  1e-01, 1e-04, 100 )
# Uncomment below for Bayes but comment above
#    strategy: BayesSearch
#    param_grid:
#      penalty: Categorical( [ 'l1', 'l2' ] )
#      C: Real( 1e-04, 1e+04 )
#      solver: Categorical( [ 'saga', 'liblinear' ] )
#      tol: Real( 1e-04, 1e-01 )

RandomForestModel:
  evaluate:
    flag: True
    evaluator_config:
      explainability_algorithm: permutation
      metric_prefix: evaluation_
  fine_tune:
    flag: True
    strategy: RandomSearch
    param_grid:
      n_estimators: np.arange(10, 101, 10)
      max_features: np.arange(6, 31, 5)
      criterion: list(['gini', 'entropy'])
      max_depth: 2**np.arange(2,8,1)

LightGBMModel:
  evaluate:
    flag: True
    evaluator_config:
      explainability_algorithm: permutation
      metric_prefix: evaluation_
  fine_tune:
    flag: True
    strategy: RandomSearch
    param_grid:
      learning_rate: np.linspace(1e-01, 1e-04, 100)
      n_estimators: np.arange(50, 151, 50)
      max_depth: np.arange(4, 11, 2)
      colsample_bytree: np.arange(0.7, 0.9, 0.1)
      subsample: np.arange(0.7, 0.9, 0.1)
      min_child_samples: np.arange( 1, 16, 5)
  creation:
    boosting_type: gbdt
    objective: binary
    num_iterations: 2000
    learning_rate: 0.01
    metric: auc

NeuralNetworkModel:
 creation:
   layers_units: [ 8 , 16 , 8 ]
   activation: relu
   dropout: 0.3
 compilation:
   optimizer: adam
   loss: binary_crossentropy
   metrics: [ accuracy ]
 fit:
   batch_size: 64
   epochs: 10
   use_multiprocessing: True
 fine_tune:
   flag: False
