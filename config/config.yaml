#LogisticRegressionModel:
# fine_tune:
#   flag: False
#   strategy: BayesSearch
#   param_grid:
#     penalty: [ l2 ]
#     C: [ 1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03, 4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02, 2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00, 1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02, 5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04 ]

#RandomForestModel:
#  fine_tune:
#    flag: False
#    strategy: RandomSearch
#    param_grid:
#      n_estimators: [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]
#      max_features: [ 6, 11, 16, 21, 26, 31 ]
#
#LightgbmModel:
#  fine_tune:
#    flag: False
#    strategy: RandomSearch
#    param_grid:
#      learning_rate: [ 0.1, 0.01 ]
#      n_estimators: [ 50, 100, 150 ]
#      max_depth: [ 4, 6 ]
#      colsample_bytree: [ 0.7, 0.8, 0.9 ]
#      subsample: [ 0.7, 0.8, 0.9 ]
#      min_child_samples: [ 1, 5, 10 ]
#  creation:
#    boosting_type: gbdt
#    objective: binary
#    num_iterations: 2000
#    learning_rate: 0.01
#    metric: auc
#
NeuralNetworkModel:
  creation:
    layers_units: [ 8 , 16 , 8 ]
    activation: relu
    dropout: 0.3
  compilation:
    optimizer: adam
    loss: binary_crossentropy
    metrics: [ accuracy ]
  fit:
    batch_size: 64
    epochs: 1
    use_multiprocessing: True
